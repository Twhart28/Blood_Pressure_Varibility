"""Utilities for exploring beat-to-beat arterial pressure waveforms.

This module provides a simple interactive experiment that loads a text
file exported from a continuous blood pressure monitor, extracts the
`reBAP` channel, and attempts to automatically detect systolic,
diastolic, and mean arterial pressure (MAP) values for each beat.

Usage
-----
Run the module directly to open a file picker and visualise a recording::

    python Beat-to-beat_test.py [optional/path/to/file.txt]

The script will plot the waveform and overlay the detected beat
landmarks.  It also prints summary statistics for the detected beats and
flags potential artefacts using simple heuristics.

Command-line flags allow overriding the column separator used when
reading whitespace-heavy exports (``--separator``) and limiting the
number of raw samples rendered in the interactive plot
(``--plot-max-points``) to speed up large-file analysis.
"""

from __future__ import annotations

import csv
import math
import re
import sys
import argparse
from dataclasses import dataclass, field
from pathlib import Path
from typing import Dict, List, Optional, Sequence, Tuple

try:
    import tkinter as tk
    from tkinter import filedialog, messagebox, ttk
except Exception:  # pragma: no cover - tkinter may be unavailable in some envs.
    tk = None
    filedialog = None
    messagebox = None
    ttk = None

try:
    import matplotlib
    if tk is not None:
        preferred_backend = "TkAgg"
    else:
        preferred_backend = "Agg"
    current_backend = matplotlib.get_backend().lower()
    if current_backend != preferred_backend.lower():
        try:
            matplotlib.use(preferred_backend, force=True)
        except Exception:
            if current_backend.startswith("qt"):
                matplotlib.use("Agg", force=True)
    import matplotlib.pyplot as plt
except ModuleNotFoundError:  # pragma: no cover - optional dependency
    plt = None

try:
    import numpy as np
except ModuleNotFoundError as exc:  # pragma: no cover - dependency guard
    raise SystemExit("numpy is required to run this script. Please install it and retry.") from exc

try:  # pragma: no cover - optional acceleration for rolling statistics
    from numpy.lib.stride_tricks import sliding_window_view
except Exception:  # pragma: no cover - fallback when unavailable
    sliding_window_view = None  # type: ignore[assignment]

try:
    import pandas as pd
    from pandas import DataFrame
except ModuleNotFoundError as exc:  # pragma: no cover - dependency guard
    raise SystemExit("pandas is required to run this script. Please install it and retry.") from exc

try:  # pragma: no cover - optional dependency for fast CSV ingestion
    import pyarrow as pa
    import pyarrow.csv as pa_csv
except ModuleNotFoundError:  # pragma: no cover - optional dependency
    pa = None  # type: ignore[assignment]
    pa_csv = None  # type: ignore[assignment]

try:  # pragma: no cover - optional fast moving-window statistics
    import bottleneck as bn
except ModuleNotFoundError:  # pragma: no cover - optional dependency
    bn = None  # type: ignore[assignment]

try:  # pragma: no cover - optional dependency for zero-phase filtering/PSD
    from scipy import signal as sp_signal
except ModuleNotFoundError:  # pragma: no cover - optional dependency
    sp_signal = None


@dataclass
class Beat:
    """Container describing a detected beat."""

    systolic_time: float
    systolic_pressure: float
    diastolic_time: float
    diastolic_pressure: float
    notch_time: Optional[float]
    map_time: float
    map_pressure: float
    rr_interval: Optional[float]
    systolic_prominence: float
    is_artifact: bool
    artifact_reasons: List[str] = field(default_factory=list)
    is_post_calibration_recovery: bool = False


@dataclass
class ArtifactConfig:
    """Tunable thresholds used to classify artefactual beats."""

    systolic_mad_multiplier: float = 4.5
    diastolic_mad_multiplier: float = 4.5
    rr_mad_multiplier: float = 3.5
    systolic_abs_floor: float = 15.0
    diastolic_abs_floor: float = 12.0
    pulse_pressure_min: float = 10.0
    rr_bounds: Tuple[float, float] = (0.3, 2.0)
    min_prominence: float = 8.0
    prominence_noise_factor: float = 0.18
    max_missing_gap: float = 10.0


@dataclass
class BPFilePreview:
    """Lightweight summary of an export gathered without full ingestion."""

    metadata: Dict[str, str]
    column_names: List[str]
    preview: Optional[DataFrame]
    skiprows: List[int]
    detected_separator: Optional[str]
    raw_lines: List[str]
    preview_rows: int = 200


@dataclass
class ImportDialogResult:
    """Selections made in the delimiter/column configuration dialog."""

    preview: BPFilePreview
    separator: Optional[str]
    time_column: str
    pressure_column: str
    header_row: int
    first_data_row: int
    time_column_index: int
    pressure_column_index: int
    comment_column: Optional[str] = None
    comment_column_index: Optional[int] = None


class FastParserError(RuntimeError):
    """Raised when the fast column loader fails to parse the file."""

    def __init__(self, message: str):
        super().__init__(message)
        self.message = message


def _tokenise_preview_line(line: str, separator: Optional[str]) -> List[str]:
    """Split a raw preview line using the provided separator."""

    if not line:
        return []

    if separator is None:
        stripped = line.strip()
        if not stripped:
            return []
        return [tok for tok in re.split(r"\s+", stripped) if tok]

    if separator == " ":
        stripped = line.strip()
        if not stripped:
            return []
        return [tok for tok in re.split(r"\s+", stripped) if tok]

    try:
        return next(csv.reader([line], delimiter=separator))
    except Exception:
        return line.split(separator)


def _scan_bp_file(
    file_path: Path,
    max_preview_rows: int = 200,
    *,
    separator_override: Optional[str] = None,
) -> BPFilePreview:
    """Read a lightweight preview without inferring structure."""

    metadata: Dict[str, str] = {}
    skiprows: List[int] = []

    preview_line_cap = max(max_preview_rows, 200)
    raw_lines: List[str] = []
    with file_path.open("r", encoding="utf-8", errors="ignore") as handle:
        for _ in range(preview_line_cap):
            line = handle.readline()
            if not line:
                break
            raw_lines.append(line.rstrip("\r\n"))

    meaningful_lines = [line for line in raw_lines if line.strip() and not line.strip().startswith("#")]

    detected_separator: Optional[str] = separator_override
    if detected_separator is None:
        sample = meaningful_lines[:20]

        def _consistent_split(delimiter: Optional[str]) -> bool:
            if not sample:
                return False
            lengths: List[int] = []
            has_multiple = False
            for line in sample:
                tokens = _tokenise_preview_line(line, delimiter)
                if not tokens:
                    return False
                if len(tokens) > 1:
                    has_multiple = True
                lengths.append(len(tokens))
            return has_multiple and len(set(lengths)) == 1

        for delimiter in ("\t", ",", ";", "|"):
            if _consistent_split(delimiter):
                detected_separator = delimiter
                break

        if detected_separator is None and _consistent_split(" "):
            detected_separator = " "

    preview_rows = max_preview_rows if max_preview_rows > 0 else 200

    column_names: List[str] = []
    lines_for_tokens = meaningful_lines if meaningful_lines else raw_lines
    if lines_for_tokens:
        effective_separator = detected_separator if detected_separator is not None else None
        column_count = 0
        for line in lines_for_tokens:
            tokens = _tokenise_preview_line(line, effective_separator)
            if tokens:
                column_count = len(tokens)
                break
        if column_count:
            column_names = [f"column_{idx + 1}" for idx in range(column_count)]

    return BPFilePreview(
        metadata=metadata,
        column_names=column_names,
        preview=None,
        skiprows=skiprows,
        detected_separator=detected_separator,
        raw_lines=raw_lines,
        preview_rows=preview_rows,
    )


def _build_preview_dataframe(
    file_path: Path,
    *,
    column_names: Sequence[str],
    skiprows: Sequence[int],
    separator: Optional[str],
    max_rows: int,
) -> DataFrame:
    """Construct a small DataFrame preview for the selected layout."""

    read_kwargs = dict(
        filepath_or_buffer=file_path,
        header=None,
        names=list(column_names),
        skiprows=list(skiprows),
        nrows=max_rows,
        na_values=["nan", "NaN", "NA"],
    )

    if separator:
        try:
            engine = "c" if len(separator) == 1 else "python"
            return pd.read_csv(sep=separator, engine=engine, **read_kwargs)
        except Exception:
            return pd.read_csv(delim_whitespace=True, engine="python", **read_kwargs)

    return pd.read_csv(delim_whitespace=True, engine="python", **read_kwargs)


def _default_column_selection(column_names: Sequence[str], requested: Optional[str], *, fallback: Optional[str] = None) -> str:
    """Return a sensible default column name."""

    if requested and requested in column_names:
        return requested

    if fallback and fallback in column_names:
        return fallback

    if column_names:
        return column_names[0]

    raise ValueError("No columns available for selection.")


def _render_preview_to_text_widget(
    widget: tk.Text,
    preview_rows: Sequence[Sequence[str]],
    *,
    header_row_index: int,
    first_data_row_index: int,
    time_column_index: Optional[int],
    pressure_column_index: Optional[int],
    max_rows: int = 200,
) -> None:
    """Render a preview table into a Tkinter text widget with highlights."""

    if tk is None:
        return

    widget.configure(state="normal")
    widget.delete("1.0", "end")

    if not preview_rows:
        widget.insert("1.0", "No preview data available.")
        widget.configure(state="disabled")
        return

    total_rows = len(preview_rows)
    display_rows = list(preview_rows[:max_rows])
    column_count = max((len(row) for row in display_rows), default=0)
    row_number_width = max(3, len(str(min(total_rows, max_rows)))) + 1

    widths: List[int] = []
    for col_idx in range(column_count):
        col_values = [
            str(row[col_idx]) if col_idx < len(row) and row[col_idx] is not None else ""
            for row in display_rows
        ]
        widths.append(max((len(value) for value in col_values), default=0))

    header_line_parts = [f"{'#':>{row_number_width}} "]
    for col_idx in range(column_count):
        label = str(col_idx + 1)
        header_line_parts.append(label.ljust(widths[col_idx] + 2))
    header_text = "".join(header_line_parts)
    widget.insert("end", header_text + "\n")
    widget.tag_add("column_index", "1.0", "1.end")

    line_no = 2
    for row_number, row_values in enumerate(display_rows, start=1):
        line_parts: List[str] = [f"{row_number:>{row_number_width}} "]
        cursor = len(line_parts[0])
        positions: List[Tuple[int, int, int]] = []

        for col_idx in range(column_count):
            value = ""
            if col_idx < len(row_values) and row_values[col_idx] is not None:
                value = str(row_values[col_idx])
            padded = value.ljust(widths[col_idx] + 2)
            start = cursor
            end = cursor + len(padded)
            positions.append((start, end, col_idx))
            line_parts.append(padded)
            cursor = end

        line_text = "".join(line_parts)
        widget.insert("end", line_text + "\n")

        row_tag = "row_even" if row_number % 2 == 0 else "row_odd"
        widget.tag_add(row_tag, f"{line_no}.0", f"{line_no}.end")

        if row_number == header_row_index:
            widget.tag_add("header", f"{line_no}.0", f"{line_no}.end")
        if row_number == first_data_row_index:
            widget.tag_add("data_start", f"{line_no}.0", f"{line_no}.end")

        for start, end, col_idx in positions:
            if time_column_index is not None and col_idx == time_column_index:
                widget.tag_add("highlight_time", f"{line_no}.{start}", f"{line_no}.{end}")
            if pressure_column_index is not None and col_idx == pressure_column_index:
                widget.tag_add("highlight_pressure", f"{line_no}.{start}", f"{line_no}.{end}")

        line_no += 1

    widget.configure(state="disabled")
    widget.tag_raise("column_index")
    widget.tag_raise("highlight_time")
    widget.tag_raise("highlight_pressure")
    widget.tag_raise("data_start")
    widget.tag_raise("header")


def launch_import_configuration_dialog(
    file_path: Path,
    preview: BPFilePreview,
    *,
    time_default: str,
    pressure_default: str,
    separator_override: Optional[str] = None,
) -> Optional[ImportDialogResult]:
    """Display a dialog allowing the user to adjust delimiter and column selections."""

    if tk is None or ttk is None:
        return None

    delimiter_options: List[Tuple[str, Optional[str]]] = [
        ("Auto-detect", None),
        ("Tab (\\t)", "\t"),
        ("Comma (,)", ","),
        ("Semicolon (;)", ";"),
        ("Pipe (|)", "|"),
        ("Space", " "),
        ("Colon (:)", ":"),
    ]

    def _label_for_separator(value: Optional[str]) -> str:
        for label, candidate in delimiter_options:
            if value == candidate:
                return label
        return "Auto-detect"

    def _resolve_separator_from_label(label: str) -> Optional[str]:
        for option_label, value in delimiter_options:
            if option_label == label:
                return value
        return None

    def _effective_separator(value: Optional[str]) -> Optional[str]:
        if value is not None:
            return value
        return preview.detected_separator

    raw_lines = list(preview.raw_lines)
    if not raw_lines:
        with file_path.open("r", encoding="utf-8", errors="ignore") as handle:
            for _ in range(200):
                line = handle.readline()
                if not line:
                    break
                raw_lines.append(line.rstrip("\r\n"))

    def _split_line(line: str, separator: Optional[str]) -> List[str]:
        return _tokenise_preview_line(line, separator)

    def _split_preview_lines(separator: Optional[str]) -> List[List[str]]:
        effective = _effective_separator(separator)
        return [_split_line(line, effective) for line in raw_lines]

    initial_separator = separator_override if separator_override is not None else preview.detected_separator
    current_rows = _split_preview_lines(initial_separator)

    if preview.skiprows:
        header_default = max(1, max(preview.skiprows) + 1)
    else:
        header_default = 1
    max_available_rows = max(len(raw_lines), 1)
    header_default = min(header_default, max_available_rows)
    first_data_default = header_default + 1 if header_default < max_available_rows else header_default + 1

    default_time_index = 1
    default_pressure_index = 2 if len(preview.column_names) >= 2 else 1
    if preview.column_names:
        if time_default in preview.column_names:
            default_time_index = preview.column_names.index(time_default) + 1
        if pressure_default in preview.column_names:
            default_pressure_index = preview.column_names.index(pressure_default) + 1
    if default_pressure_index == default_time_index:
        if len(preview.column_names) >= 2:
            for idx in range(1, len(preview.column_names) + 1):
                if idx != default_time_index:
                    default_pressure_index = idx
                    break
        if default_pressure_index == default_time_index:
            default_pressure_index = max(1, min(default_time_index + 1, len(preview.column_names) or 1))

    root = tk.Tk()
    root.title("Import options")
    root.geometry("960x640")

    main_frame = ttk.Frame(root, padding=16)
    main_frame.grid(row=0, column=0, sticky="nsew")
    root.columnconfigure(0, weight=1)
    root.rowconfigure(0, weight=1)
    main_frame.columnconfigure(0, weight=1)
    main_frame.rowconfigure(2, weight=1)

    controls_frame = ttk.Frame(main_frame)
    controls_frame.grid(row=0, column=0, sticky="ew", pady=(0, 12))
    controls_frame.columnconfigure(6, weight=1)

    ttk.Label(controls_frame, text="Delimiter:").grid(row=0, column=0, sticky="w")
    delimiter_var = tk.StringVar(value=_label_for_separator(initial_separator))
    delimiter_combo = ttk.Combobox(
        controls_frame,
        state="readonly",
        values=[label for label, _ in delimiter_options],
        textvariable=delimiter_var,
        width=18,
    )
    delimiter_combo.grid(row=0, column=1, sticky="w", padx=(8, 24))

    ttk.Label(controls_frame, text="Header row:").grid(row=0, column=2, sticky="w")
    header_var = tk.IntVar(value=header_default)
    header_spin = tk.Spinbox(
        controls_frame,
        from_=1,
        to=max(max_available_rows, header_default + 100),
        textvariable=header_var,
        width=8,
    )
    header_spin.grid(row=0, column=3, sticky="w", padx=(8, 24))

    ttk.Label(controls_frame, text="First data row:").grid(row=0, column=4, sticky="w")
    first_data_var = tk.IntVar(value=first_data_default)
    first_data_spin = tk.Spinbox(
        controls_frame,
        from_=1,
        to=max(max_available_rows, first_data_default + 100),
        textvariable=first_data_var,
        width=8,
    )
    first_data_spin.grid(row=0, column=5, sticky="w")

    ttk.Label(controls_frame, text="Time column:").grid(row=1, column=0, sticky="w", pady=(12, 0))
    time_var = tk.StringVar()
    time_combo = ttk.Combobox(
        controls_frame,
        state="readonly",
        textvariable=time_var,
        width=24,
    )
    time_combo.grid(row=1, column=1, sticky="w", padx=(8, 24), pady=(12, 0))

    ttk.Label(controls_frame, text="Pressure column:").grid(row=1, column=2, sticky="w", pady=(12, 0))
    pressure_var = tk.StringVar()
    pressure_combo = ttk.Combobox(
        controls_frame,
        state="readonly",
        textvariable=pressure_var,
        width=24,
    )
    pressure_combo.grid(row=1, column=3, sticky="w", padx=(8, 24), pady=(12, 0))

    comment_enabled_var = tk.BooleanVar(value=False)
    comment_index_var = tk.StringVar(value="")

    def _on_comment_toggle(*_: object) -> None:
        enabled = bool(comment_enabled_var.get())
        state = "normal" if enabled else "disabled"
        comment_entry.configure(state=state)
        if not enabled:
            comment_index_var.set("")
        _update_column_options()
        _update_preview_widget()

    def _on_comment_index_change(*_: object) -> None:
        _update_column_options()
        _update_preview_widget()

    ttk.Checkbutton(
        controls_frame,
        text="Include comment column",
        variable=comment_enabled_var,
        command=_on_comment_toggle,
    ).grid(row=2, column=0, columnspan=2, sticky="w", pady=(12, 0))

    ttk.Label(controls_frame, text="Column #:").grid(row=2, column=2, sticky="w", pady=(12, 0))
    comment_entry = ttk.Entry(
        controls_frame,
        width=8,
        textvariable=comment_index_var,
        state="disabled",
    )
    comment_entry.grid(row=2, column=3, sticky="w", padx=(8, 24), pady=(12, 0))

    initial_comment_index: Optional[int] = None
    for idx, name in enumerate(preview.column_names, start=1):
        if str(name).strip().lower() == "comment":
            initial_comment_index = idx
            break
    if initial_comment_index is not None:
        comment_enabled_var.set(True)
        comment_entry.configure(state="normal")
        comment_index_var.set(str(initial_comment_index))

    ttk.Label(
        main_frame,
        text="Data preview (first 200 rows)",
        font=("TkDefaultFont", 10, "bold"),
    ).grid(row=1, column=0, sticky="w")

    preview_frame = ttk.Frame(main_frame, relief=tk.SOLID, borderwidth=1)
    preview_frame.grid(row=2, column=0, sticky="nsew")
    preview_frame.columnconfigure(0, weight=1)
    preview_frame.rowconfigure(0, weight=1)

    preview_text = tk.Text(
        preview_frame,
        wrap="none",
        font=("Courier New", 10),
        height=20,
    )
    preview_text.grid(row=0, column=0, sticky="nsew")
    preview_text.configure(cursor="arrow")
    preview_text.bind("<Key>", lambda _: "break")

    y_scroll = ttk.Scrollbar(preview_frame, orient="vertical", command=preview_text.yview)
    y_scroll.grid(row=0, column=1, sticky="ns")
    x_scroll = ttk.Scrollbar(preview_frame, orient="horizontal", command=preview_text.xview)
    x_scroll.grid(row=1, column=0, sticky="ew")
    preview_text.configure(yscrollcommand=y_scroll.set, xscrollcommand=x_scroll.set)

    preview_text.tag_configure("column_index", background="#ccd6f6", font=("Courier New", 10, "bold"))
    preview_text.tag_configure("header", background="#d9d9d9", font=("Courier New", 10, "bold"))
    preview_text.tag_configure("row_even", background="#f5f5f5")
    preview_text.tag_configure("row_odd", background="#ededed")
    preview_text.tag_configure("highlight_time", background="#dbeafe")
    preview_text.tag_configure("highlight_pressure", background="#fdeac5")
    preview_text.tag_configure("data_start", background="#e7f5e4")

    error_var = tk.StringVar(value="")
    error_label = ttk.Label(main_frame, textvariable=error_var, foreground="red")
    error_label.grid(row=3, column=0, sticky="w", pady=(8, 0))

    button_frame = ttk.Frame(main_frame)
    button_frame.grid(row=4, column=0, sticky="e", pady=(16, 0))

    result: Dict[str, object] = {}
    current_preview_state: Dict[str, object] = {
        "preview": preview,
        "separator": initial_separator,
        "resolved_separator": _effective_separator(initial_separator),
        "rows": current_rows,
        "header_row": header_default,
        "first_data_row": first_data_default,
        "preview_rows": preview.preview_rows,
    }

    column_option_map: Dict[str, int] = {}

    def _current_preview() -> BPFilePreview:
        stored = current_preview_state.get("preview")
        if isinstance(stored, BPFilePreview):
            return stored
        return preview

    def _current_rows() -> List[List[str]]:
        rows = current_preview_state.get("rows")
        if isinstance(rows, list):
            return rows
        return current_rows

    def _parse_column_index(value: str) -> Optional[int]:
        if not value:
            return None
        if value in column_option_map:
            return column_option_map[value]
        prefix = value.split(":", 1)[0].strip()
        if prefix.isdigit():
            return int(prefix)
        return None

    def _parse_comment_index() -> Optional[int]:
        if not comment_enabled_var.get():
            return None
        raw = comment_index_var.get().strip()
        if not raw:
            return None
        if not raw.isdigit():
            return None
        value = int(raw)
        if value <= 0:
            return None
        return value

    def _get_header_row() -> int:
        try:
            value = int(header_var.get())
        except (tk.TclError, ValueError):
            value = header_default
        return max(1, value)

    def _get_first_data_row() -> int:
        try:
            value = int(first_data_var.get())
        except (tk.TclError, ValueError):
            value = first_data_default
        return max(1, value)

    def _build_column_options() -> List[Tuple[str, int]]:
        rows = _current_rows()
        header_index = _get_header_row()
        if rows:
            header_index = min(header_index, len(rows))
        header_values: Sequence[str] = []
        if 1 <= header_index <= len(rows):
            header_values = rows[header_index - 1]
        sample_rows = rows[:50]
        column_count = max((len(row) for row in sample_rows), default=len(preview.column_names))
        column_count = max(column_count, len(header_values))
        comment_index = _parse_comment_index()
        if comment_index:
            column_count = max(column_count, comment_index)
        if column_count == 0:
            column_count = len(preview.column_names) or 1
        options: List[Tuple[str, int]] = []
        for idx in range(column_count):
            header_label = ""
            if idx < len(header_values):
                raw_value = header_values[idx]
                header_label = str(raw_value).strip() if raw_value is not None else ""
            if not header_label:
                if comment_index and (idx + 1) == comment_index:
                    header_label = "Comment"
                else:
                    header_label = f"Column {idx + 1}"
            options.append((f"{idx + 1}: {header_label}", idx + 1))
        return options

    def _select_label_for_index(options: Sequence[Tuple[str, int]], desired_index: int) -> str:
        for label, idx in options:
            if idx == desired_index:
                return label
        return options[0][0] if options else ""

    def _update_column_options(initial: bool = False) -> None:
        nonlocal column_option_map
        options = _build_column_options()
        column_option_map = {label: idx for label, idx in options}
        labels = [label for label, _ in options]
        time_combo.configure(values=labels)
        pressure_combo.configure(values=labels)

        if not options:
            time_var.set("")
            pressure_var.set("")
            return

        current_time_index = _parse_column_index(time_var.get()) or default_time_index
        current_pressure_index = _parse_column_index(pressure_var.get()) or default_pressure_index

        if initial:
            current_time_index = default_time_index
            current_pressure_index = default_pressure_index

        current_time_index = max(1, min(current_time_index, options[-1][1]))
        current_pressure_index = max(1, min(current_pressure_index, options[-1][1]))

        if current_pressure_index == current_time_index and len(options) > 1:
            for _, idx in options:
                if idx != current_time_index:
                    current_pressure_index = idx
                    break

        time_var.set(_select_label_for_index(options, current_time_index))
        pressure_var.set(_select_label_for_index(options, current_pressure_index))

    def _get_time_index() -> Optional[int]:
        return _parse_column_index(time_var.get())

    def _get_pressure_index() -> Optional[int]:
        return _parse_column_index(pressure_var.get())

    def _get_comment_index() -> Optional[int]:
        return _parse_comment_index()

    def _update_preview_widget() -> None:
        rows = _current_rows()
        header_index = _get_header_row()
        data_index = _get_first_data_row()
        time_index = _get_time_index()
        pressure_index = _get_pressure_index()
        preview_limit = current_preview_state.get("preview_rows")
        if not isinstance(preview_limit, int) or preview_limit <= 0:
            preview_limit = _current_preview().preview_rows
        _render_preview_to_text_widget(
            preview_text,
            rows,
            header_row_index=header_index,
            first_data_row_index=data_index,
            time_column_index=(time_index - 1) if time_index else None,
            pressure_column_index=(pressure_index - 1) if pressure_index else None,
            max_rows=preview_limit,
        )

    def _refresh_preview_from_separator(*_: object) -> None:
        selection = delimiter_var.get()
        desired_separator = _resolve_separator_from_label(selection)
        rows = _split_preview_lines(desired_separator)
        current_preview_state["rows"] = rows
        current_preview_state["separator"] = desired_separator
        current_preview_state["resolved_separator"] = _effective_separator(desired_separator)
        _update_column_options()
        _update_preview_widget()

    def _on_structure_change(*_: object) -> None:
        error_var.set("")
        current_preview_state["header_row"] = _get_header_row()
        current_preview_state["first_data_row"] = _get_first_data_row()
        _update_column_options()
        _update_preview_widget()

    def _read_tokens_for_line(line_number: int, separator: Optional[str]) -> List[str]:
        if line_number <= len(raw_lines):
            return _split_line(raw_lines[line_number - 1], separator)
        with file_path.open("r", encoding="utf-8", errors="ignore") as handle:
            for idx, raw_line in enumerate(handle, start=1):
                if idx == line_number:
                    return _split_line(raw_line.rstrip("\r\n"), separator)
        return []

    def _derive_column_names(
        header_tokens: Sequence[str],
        column_count: int,
        *,
        comment_index: Optional[int] = None,
    ) -> List[str]:
        names: List[str] = []
        seen: Dict[str, int] = {}
        for idx in range(column_count):
            raw_value = header_tokens[idx] if idx < len(header_tokens) else ""
            candidate = str(raw_value).strip() if raw_value is not None else ""
            if not candidate:
                if comment_index and (idx + 1) == comment_index:
                    candidate = "Comment"
                else:
                    candidate = f"column_{idx + 1}"
            base = candidate
            suffix = 1
            while candidate in seen:
                suffix += 1
                candidate = f"{base}_{suffix}"
            seen[candidate] = suffix
            names.append(candidate)
        return names

    def _confirm() -> None:
        header_index = _get_header_row()
        data_index = _get_first_data_row()
        time_index = _get_time_index()
        pressure_index = _get_pressure_index()
        comment_index = _get_comment_index()

        if time_index is None or pressure_index is None:
            error_var.set("Select both time and pressure columns.")
            return
        if time_index == pressure_index:
            error_var.set("Time and pressure selections must differ.")
            return
        if comment_enabled_var.get() and comment_index is None:
            error_var.set("Enter a valid comment column number.")
            return
        if data_index <= header_index:
            error_var.set("First data row must be after the header row.")
            return

        rows = _current_rows()
        resolved_candidate = current_preview_state.get("resolved_separator")
        if isinstance(resolved_candidate, str) or resolved_candidate is None:
            resolved_separator = resolved_candidate
        else:
            resolved_separator = _effective_separator(current_preview_state.get("separator"))

        header_tokens = _read_tokens_for_line(header_index, resolved_separator)
        sample_rows = rows[:50]
        column_count = max((len(row) for row in sample_rows), default=0)
        column_count = max(column_count, len(header_tokens), time_index, pressure_index)
        if comment_index:
            column_count = max(column_count, comment_index)
        if column_count <= 0:
            column_count = max(time_index, pressure_index)

        column_names = _derive_column_names(
            header_tokens,
            column_count,
            comment_index=comment_index,
        )
        if time_index > len(column_names) or pressure_index > len(column_names):
            error_var.set("Selected columns exceed detected column count.")
            return
        if comment_index and comment_index > len(column_names):
            error_var.set("Comment column exceeds detected column count.")
            return

        skiprows = set(preview.skiprows)
        skiprows.update(range(max(0, data_index - 1)))
        skiprows_list = sorted(skiprows)

        preview_row_limit = current_preview_state.get("preview_rows")
        if isinstance(preview_row_limit, int) and preview_row_limit > 0:
            max_rows = preview_row_limit
        else:
            max_rows = preview.preview_rows

        try:
            preview_df = _build_preview_dataframe(
                file_path,
                column_names=column_names,
                skiprows=skiprows_list,
                separator=resolved_separator,
                max_rows=max_rows,
            )
        except Exception as exc:  # pragma: no cover - interactive feedback
            error_var.set(f"Failed to build preview: {exc}")
            return

        updated_preview = BPFilePreview(
            metadata=preview.metadata,
            column_names=column_names,
            preview=preview_df,
            skiprows=skiprows_list,
            detected_separator=resolved_separator,
            raw_lines=raw_lines,
            preview_rows=max_rows,
        )

        current_preview_state["preview"] = updated_preview
        current_preview_state["header_row"] = header_index
        current_preview_state["first_data_row"] = data_index
        current_preview_state["preview_rows"] = max_rows

        result["time"] = column_names[time_index - 1]
        result["pressure"] = column_names[pressure_index - 1]
        result["time_index"] = time_index
        result["pressure_index"] = pressure_index
        if comment_index:
            result["comment_index"] = comment_index
            result["comment_name"] = column_names[comment_index - 1]
        else:
            result["comment_index"] = None
            result["comment_name"] = None
        result["separator"] = current_preview_state.get("separator")
        result["preview"] = updated_preview
        result["header_row"] = header_index
        result["first_data_row"] = data_index
        error_var.set("")
        root.quit()

    def _cancel() -> None:
        result.clear()
        root.quit()

    _update_column_options(initial=True)
    _update_preview_widget()

    delimiter_var.trace_add("write", _refresh_preview_from_separator)
    header_var.trace_add("write", _on_structure_change)
    first_data_var.trace_add("write", _on_structure_change)
    time_var.trace_add("write", lambda *_: _update_preview_widget())
    pressure_var.trace_add("write", lambda *_: _update_preview_widget())
    comment_index_var.trace_add("write", _on_comment_index_change)

    ttk.Button(button_frame, text="Cancel", command=_cancel).grid(row=0, column=0, padx=(0, 8))
    ttk.Button(button_frame, text="Import", command=_confirm).grid(row=0, column=1)

    root.protocol("WM_DELETE_WINDOW", _cancel)

    root.mainloop()
    root.destroy()

    required_keys = {
        "time",
        "pressure",
        "preview",
        "separator",
        "header_row",
        "first_data_row",
        "time_index",
        "pressure_index",
    }
    if required_keys.issubset(result.keys()):
        return ImportDialogResult(
            preview=result["preview"],
            separator=result.get("separator"),
            time_column=str(result["time"]),
            pressure_column=str(result["pressure"]),
            header_row=int(result["header_row"]),
            first_data_row=int(result["first_data_row"]),
            time_column_index=int(result["time_index"]),
            pressure_column_index=int(result["pressure_index"]),
            comment_column=(
                str(result["comment_name"])
                if result.get("comment_name") not in (None, "")
                else None
            ),
            comment_column_index=(
                int(result["comment_index"])
                if isinstance(result.get("comment_index"), int)
                else None
            ),
        )

    return None


def _prompt_fast_parser_decision(*, title: str, summary: str, details: str) -> bool:
    """Display a dialog explaining why the fast parser is unavailable or failed."""

    message = (
        f"{summary}\n\n"
        f"Details:\n{details.strip()}\n\n"
        "Would you like to continue with the slower parser?"
    )

    if tk is not None and messagebox is not None:
        dialog_root: Optional[tk.Tk] = None
        try:
            dialog_root = tk.Tk()
            dialog_root.withdraw()
            result = messagebox.askyesno(
                title,
                message,
                parent=dialog_root,
            )
            return bool(result)
        except Exception:
            pass
        finally:
            if dialog_root is not None:
                dialog_root.destroy()

    print(
        f"{title}: {summary} "
        f"Details: {details.strip()} "
        "Proceeding with the slower parser."
    )
    return True


def _select_columns_via_cli(
    column_names: Sequence[str],
    *,
    default_time: str,
    default_pressure: str,
) -> Tuple[str, str]:
    """Prompt for time/pressure selection using stdin."""

    if not sys.stdin.isatty():
        return default_time, default_pressure

    print("\nSelect time and pressure columns (press Enter to accept defaults).")
    for idx, name in enumerate(column_names):
        marker = ""
        if name == default_time:
            marker = " (default time)"
        elif name == default_pressure:
            marker = " (default pressure)"
        print(f"  [{idx}] {name}{marker}")

    def _prompt(label: str, default: str) -> str:
        while True:
            response = input(f"{label} [{default}]: ").strip()
            if not response:
                return default
            if response in column_names:
                return response
            if response.isdigit():
                idx = int(response)
                if 0 <= idx < len(column_names):
                    return column_names[idx]
            print("Invalid selection. Choose a column name or index.")

    time_column = _prompt("Time column", default_time)
    while True:
        pressure_column = _prompt("Pressure column", default_pressure)
        if pressure_column == time_column:
            print("Pressure column must differ from the time column.")
        else:
            break

    return time_column, pressure_column


def _select_columns_via_tk(
    column_names: Sequence[str],
    *,
    default_time: str,
    default_pressure: str,
) -> Tuple[str, str]:
    """Display a Tkinter dialog for column selection."""

    if tk is None:
        return default_time, default_pressure

    root = tk.Tk()
    root.title("Select waveform columns")
    root.geometry("360x180")

    time_var = tk.StringVar(value=default_time)
    pressure_var = tk.StringVar(value=default_pressure)
    error_var = tk.StringVar(value="")

    tk.Label(root, text="Time column:").pack(pady=(12, 0))
    tk.OptionMenu(root, time_var, *column_names).pack()

    tk.Label(root, text="Pressure column:").pack(pady=(8, 0))
    tk.OptionMenu(root, pressure_var, *column_names).pack()

    error_label = tk.Label(root, textvariable=error_var, fg="red")
    error_label.pack(pady=(6, 0))

    selection: Dict[str, str] = {}

    def confirm() -> None:
        time_choice = time_var.get()
        pressure_choice = pressure_var.get()
        if time_choice == pressure_choice:
            error_var.set("Time and pressure must differ.")
            return
        selection["time"] = time_choice
        selection["pressure"] = pressure_choice
        root.quit()

    tk.Button(root, text="Confirm", command=confirm).pack(pady=12)

    root.mainloop()
    root.destroy()

    if "time" in selection and "pressure" in selection:
        return selection["time"], selection["pressure"]

    return default_time, default_pressure


def select_time_pressure_columns(
    preview: BPFilePreview,
    *,
    requested_time: Optional[str] = None,
    requested_pressure: Optional[str] = None,
    allow_gui: bool = True,
) -> Tuple[str, str]:
    """Resolve the time and pressure columns, prompting the user when possible."""

    column_names = preview.column_names
    time_default = _default_column_selection(column_names, requested_time, fallback="Time")

    remaining = [name for name in column_names if name != time_default]
    pressure_default = _default_column_selection(
        remaining if remaining else column_names,
        requested_pressure if requested_pressure != time_default else None,
        fallback="reBAP",
    )

    if allow_gui and tk is not None:
        try:
            return _select_columns_via_tk(
                column_names,
                default_time=time_default,
                default_pressure=pressure_default,
            )
        except Exception:
            pass

    return _select_columns_via_cli(column_names, default_time=time_default, default_pressure=pressure_default)


def _load_selected_columns_fast(
    file_path: Path,
    *,
    column_indices: Sequence[int],
    column_names: Sequence[str],
    first_data_row: int,
    separator: Optional[str],
    skiprows: Sequence[int],
) -> DataFrame:
    """Attempt to load selected columns quickly using PyArrow's CSV reader."""

    if pa_csv is None or pa is None:
        raise FastParserError("PyArrow is not installed; fast parsing is unavailable.")

    if not column_indices:
        raise FastParserError("No column indices were provided for fast parsing.")

    try:
        resolved_indices = [int(idx) for idx in column_indices]
    except Exception as exc:
        raise FastParserError("Column indices for fast parsing must be integers.") from exc

    for idx in resolved_indices:
        if idx < 0 or idx >= len(column_names):
            raise FastParserError(
                "One or more requested column indices are outside the detected column range."
            )

    selected_column_names = [column_names[idx] for idx in resolved_indices]

    if separator is None:
        raise FastParserError("Fast parser requires a resolved delimiter to use the PyArrow reader.")

    if not separator or len(separator) != 1:
        raise FastParserError(
            "Fast parser using the PyArrow reader supports only single-character delimiters."
        )

    delimiter = separator

    combined_skiprows = set(skiprows)
    combined_skiprows.update(range(max(0, first_data_row - 1)))
    skiprows_list = sorted(combined_skiprows)

    contiguous_prefix = 0
    for expected, row_index in enumerate(skiprows_list):
        if row_index != expected:
            break
        contiguous_prefix = expected + 1

    non_contiguous_skips = skiprows_list[contiguous_prefix:]
    if non_contiguous_skips:
        raise FastParserError(
            "PyArrow's CSV reader only supports skipping an initial contiguous block of rows, "
            "but additional rows were requested: "
            f"{non_contiguous_skips}."
        )

    skip_rows_count = contiguous_prefix

    if not column_names:
        raise FastParserError("No column names were provided for fast parsing.")

    file_path_str = str(file_path)

    try:
        read_options = pa_csv.ReadOptions(
            column_names=list(column_names),
            skip_rows=skip_rows_count,
        )
        parse_options = pa_csv.ParseOptions(delimiter=delimiter)
        convert_options = pa_csv.ConvertOptions(
            include_columns=selected_column_names,
            column_types={name: pa.float32() for name in selected_column_names},
            strings_can_be_null=True,
            null_values=["", "nan", "NaN", "NA"],
        )
        table = pa_csv.read_csv(
            file_path_str,
            read_options=read_options,
            parse_options=parse_options,
            convert_options=convert_options,
        )
    except Exception as exc:
        raise FastParserError(
            "PyArrow CSV reader failed to parse the selected columns: "
            f"{exc}. Requested columns: {selected_column_names}"
        ) from exc

    if table.num_rows == 0:
        return pd.DataFrame(columns=selected_column_names)

    frame = table.to_pandas(self_destruct=True, split_blocks=True)
    frame = frame.reindex(columns=selected_column_names)

    for column in frame.columns:
        try:
            frame[column] = pd.to_numeric(frame[column], errors="coerce").astype(
                np.float32, copy=False
            )
        except Exception:
            frame[column] = frame[column]

    frame = frame.dropna(how="all")
    return frame


def load_bp_file(
    file_path: Path,
    *,
    preview: Optional[BPFilePreview] = None,
    time_column: Optional[str] = None,
    pressure_column: Optional[str] = None,
    separator: Optional[str] = None,
    import_settings: Optional[ImportDialogResult] = None,
) -> Tuple[DataFrame, Dict[str, str]]:
    """Load the selected columns from a waveform export."""

    if preview is None:
        preview = _scan_bp_file(file_path, separator_override=separator)
    elif separator:
        preview.detected_separator = separator

    if time_column is None or time_column not in preview.column_names or pressure_column is None or pressure_column not in preview.column_names:
        time_column, pressure_column = select_time_pressure_columns(
            preview,
            requested_time=time_column,
            requested_pressure=pressure_column,
        )

    desired = [time_column, pressure_column]
    # ``dict.fromkeys`` preserves order while removing duplicates.
    usecols = list(dict.fromkeys(desired))
    dtype_map = {name: np.float32 for name in usecols}

    read_kwargs = dict(
        filepath_or_buffer=file_path,
        comment="#",
        header=None,
        names=preview.column_names,
        usecols=usecols,

        skiprows=preview.skiprows,
        na_values=["nan", "NaN", "NA"],
        dtype=dtype_map,
    )

    detected_separator = preview.detected_separator
    resolved_separator = separator if separator is not None else detected_separator

    frame: Optional[DataFrame] = None
    fast_failure_reason: Optional[str] = None
    fast_unavailable_reason: Optional[str] = None

    fast_usecols: Optional[List[int]] = None
    fast_debug_indices: Optional[List[int]] = None
    if import_settings is None:
        fast_unavailable_reason = (
            "Fast parser requires the interactive import dialog to capture column indices "
            f"(time column: '{time_column}', pressure column: '{pressure_column}')."
        )
    else:
        index_lookup = {
            import_settings.time_column: import_settings.time_column_index - 1,
            import_settings.pressure_column: import_settings.pressure_column_index - 1,
        }
        fast_usecols = []
        for column_name in usecols:
            idx = index_lookup.get(column_name)
            if idx is None:
                fast_unavailable_reason = (
                    f"Column '{column_name}' does not have a recorded index from the preview dialog, "
                    "so the fast parser cannot be configured."
                )
                fast_usecols = None
                break
            fast_usecols.append(idx)

        if fast_usecols is not None:
            if resolved_separator is None:
                fast_unavailable_reason = (
                    "The fast parser requires a resolved delimiter to use the PyArrow reader."
                )
                fast_usecols = None
            elif not resolved_separator or len(resolved_separator) != 1:
                fast_unavailable_reason = (
                    "The fast parser using the PyArrow reader supports only single-character delimiters, "
                    f"but '{resolved_separator}' was selected."
                )
                fast_usecols = None

    if fast_usecols:
        fast_debug_indices = list(fast_usecols)
        try:
            frame = _load_selected_columns_fast(
                file_path,
                column_indices=fast_usecols,
                column_names=preview.column_names,
                first_data_row=import_settings.first_data_row if import_settings else 1,
                separator=resolved_separator,
                skiprows=preview.skiprows,
            )
        except FastParserError as exc:
            debug_lines = [exc.message]
            debug_lines.append(
                "Resolved separator: "
                + (repr(resolved_separator) if resolved_separator is not None else "None")
            )
            debug_lines.append(f"Requested columns: {usecols}")
            if fast_debug_indices is not None:
                debug_lines.append(f"Column indices: {fast_debug_indices}")
            fast_failure_reason = "\n".join(debug_lines)

    if frame is not None:
        return frame, preview.metadata

    if fast_failure_reason:
        if not _prompt_fast_parser_decision(
            title="Fast parser failed",
            summary="The fast parser was unable to load the selected columns.",
            details=fast_failure_reason,
        ):
            raise RuntimeError(
                "Import cancelled after fast parser failure."
            )
    elif fast_unavailable_reason:
        debug_lines = [fast_unavailable_reason]
        debug_lines.append(
            "Resolved separator: "
            + (repr(resolved_separator) if resolved_separator is not None else "None")
        )
        debug_lines.append(f"Requested columns: {usecols}")
        fast_unavailable_reason = "\n".join(debug_lines)
        if not _prompt_fast_parser_decision(
            title="Fast parser unavailable",
            summary="The fast parser could not be used for this file.",
            details=fast_unavailable_reason,
        ):
            raise RuntimeError(
                "Import cancelled because the fast parser could not be used."
            )

    if resolved_separator:
        try:
            if len(resolved_separator) == 1:
                frame = pd.read_csv(
                    sep=resolved_separator,
                    engine="c",
                    **read_kwargs,
                )
            else:
                frame = pd.read_csv(
                    sep=resolved_separator,
                    engine="python",
                    **read_kwargs,
                )
        except Exception:
            frame = pd.read_csv(
                delim_whitespace=True,
                engine="python",
                **read_kwargs,
            )
    else:
        frame = pd.read_csv(
            delim_whitespace=True,
            engine="python",
            **read_kwargs,
        )

    return frame, preview.metadata


def parse_interval(metadata: Dict[str, str], frame: DataFrame) -> float:
    """Extract the sampling interval from metadata or infer it from the data."""

    metadata_interval: Optional[float] = None
    inferred_interval: Optional[float] = None

    interval_raw = metadata.get("Interval")
    if interval_raw:
        match = re.search(r"([0-9]+\.?[0-9]*)", interval_raw)
        if match:
            metadata_interval = float(match.group(1))

    # Infer from the time column if available.
    if "Time" in frame.columns:
        time_values = frame["Time"].to_numpy()
        if len(time_values) > 1:
            diffs = np.diff(time_values)
            diffs = diffs[~np.isnan(diffs)]
            if len(diffs) > 0:
                inferred_interval = float(np.median(diffs))

    if metadata_interval and inferred_interval:
        if metadata_interval > 0 and abs(inferred_interval - metadata_interval) / metadata_interval > 0.1:
            print(
                "Warning: metadata interval"
                f" ({metadata_interval:.4f}s) differs from inferred interval"
                f" ({inferred_interval:.4f}s)."
            )
        return metadata_interval

    if metadata_interval:
        return metadata_interval

    if inferred_interval:
        return inferred_interval

    # Fallback to 1 Hz.
    return 1.0


def smooth_signal(signal: np.ndarray, window_seconds: float, fs: float) -> np.ndarray:
    """Apply a simple moving average filter to suppress high-frequency noise."""

    window_samples = max(1, int(round(window_seconds * fs)))
    if window_samples <= 1:
        return signal.astype(float, copy=False)

    kernel = np.ones(window_samples, dtype=float) / float(window_samples)
    return np.convolve(signal, kernel, mode="same")


def zero_phase_filter(
    signal: np.ndarray,
    fs: float,
    *,
    lowcut: float = 0.5,
    highcut: float = 20.0,
    order: int = 4,
) -> np.ndarray:
    """Apply a zero-phase bandpass filter when SciPy is available.

    Falls back to returning the input data when filtering fails or SciPy is
    unavailable. The cutoff range is clipped to the [0, Nyquist) interval to
    avoid numerical errors on very low/high sampling rates.
    """

    if sp_signal is None or fs <= 0:
        return signal

    nyquist = 0.5 * fs
    if nyquist <= 0:
        return signal

    low = max(0.01, lowcut) / nyquist
    high = min(highcut, nyquist * 0.99) / nyquist
    if not (0 < low < high < 1):
        return signal

    try:
        b, a = sp_signal.butter(order, [low, high], btype="bandpass")
        return sp_signal.filtfilt(b, a, signal.astype(float, copy=False))
    except Exception:
        return signal


def detrend_signal(signal: np.ndarray, fs: float, window_seconds: float = 0.6) -> np.ndarray:
    """Remove slow-varying trends using a long moving-average window."""

    if window_seconds <= 0:
        return signal

    baseline = smooth_signal(signal, window_seconds=window_seconds, fs=fs)
    return signal - baseline


def _find_long_gaps(time: np.ndarray, mask: np.ndarray, gap_seconds: float) -> List[Tuple[float, float]]:
    """Return start/stop times for gaps that exceed ``gap_seconds``."""

    if gap_seconds <= 0 or time.size == 0:
        return []

    invalid_spans: List[Tuple[float, float]] = []
    gap_start: Optional[int] = None

    for idx, missing in enumerate(mask):
        if missing and gap_start is None:
            gap_start = idx
        elif not missing and gap_start is not None:
            gap_end = idx - 1
            if gap_end >= gap_start:
                start_time = float(time[gap_start])
                end_time = float(time[min(gap_end, len(time) - 1)])
                if end_time - start_time >= gap_seconds:
                    invalid_spans.append((start_time, end_time))
            gap_start = None

    if gap_start is not None:
        start_time = float(time[gap_start])
        end_time = float(time[-1])
        if end_time - start_time >= gap_seconds:
            invalid_spans.append((start_time, end_time))

    return invalid_spans


def detect_systolic_peaks(
    signal: np.ndarray,
    fs: float,
    *,
    min_rr: float,
    max_rr: float,
    prominence_floor: float,
    prominence_noise_factor: float,
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, List[Tuple[int, int]]]:
    """Detect systolic peaks using adaptive upstroke and prominence checks.

    The Lab chart module applies a two-step process: first it emphasises the
    rapid systolic upstroke and then it validates peaks based on their
    prominence relative to neighbouring troughs.  To emulate that behaviour we
    detrend the signal, analyse the velocity profile, and adaptively gate the
    candidate peaks before evaluating their prominences.  Peak candidates are
    generated in a vectorised fashion (using :func:`scipy.signal.find_peaks`
    when available) to reduce Python-level looping while preserving the legacy
    gating heuristics.
    """

    if signal.size == 0:
        return (
            np.array([], dtype=int),
            np.array([], dtype=float),
            np.array([], dtype=int),
            [],
        )

    detrended = detrend_signal(signal, fs, window_seconds=0.6)
    velocity = np.gradient(detrended)
    velocity = smooth_signal(velocity, window_seconds=0.03, fs=fs)
    positive_velocity = np.clip(velocity, a_min=0.0, a_max=None)

    if not np.any(positive_velocity > 0):
        return (
            np.array([], dtype=int),
            np.array([], dtype=float),
            np.array([], dtype=int),
            [],
        )

    positive_samples = positive_velocity[positive_velocity > 0]
    baseline = float(np.median(positive_samples))
    mad = float(np.median(np.abs(positive_samples - baseline)))
    if mad == 0.0:
        mad = float(np.std(positive_samples))
    if mad == 0.0:
        mad = 1.0

    global_threshold = baseline + 2.5 * mad
    if global_threshold <= 0:
        global_threshold = baseline * 1.5 if baseline > 0 else 0.5

    velocity_scale = float(np.nanmedian(np.abs(velocity))) if velocity.size else 0.0
    plateau_eps = max(0.02, 0.3 * velocity_scale)
    plateau_samples = max(1, int(round(0.5 * fs)))
    calibration_segments: List[Tuple[int, int]] = []
    if plateau_samples > 1:
        mask = np.abs(velocity) <= plateau_eps
        idx = 0
        while idx < mask.size:
            if mask[idx]:
                start = idx
                while idx < mask.size and mask[idx]:
                    idx += 1
                if idx - start >= plateau_samples:
                    calibration_segments.append((start, idx))
            else:
                idx += 1

    rolling_window_seconds = 0.2
    default_window = max(3, int(round(rolling_window_seconds * fs)))
    if default_window % 2 == 0:
        default_window += 1

    local_medians = np.full(signal.size, np.nan, dtype=float)
    local_mads = np.full(signal.size, np.nan, dtype=float)

    segment_boundaries: List[Tuple[int, int]] = []
    cursor = 0
    for start, end in calibration_segments:
        if start > cursor:
            segment_boundaries.append((cursor, start))
        cursor = max(cursor, end)
    if cursor < signal.size:
        segment_boundaries.append((cursor, signal.size))

    stride = max(1, int(round(fs / 125.0)))

    for seg_start, seg_end in segment_boundaries:
        seg_len = seg_end - seg_start
        if seg_len < 3:
            continue
        window = min(default_window, seg_len if seg_len % 2 == 1 else seg_len - 1)
        if window < 3:
            continue
        segment = positive_velocity[seg_start:seg_end]
        if stride == 1:
            seg_med, seg_mad = _rolling_median_and_mad(segment, window)
        else:
            reduced_series = segment[::stride]
            if reduced_series.size < 3:
                seg_med, seg_mad = _rolling_median_and_mad(segment, window)
            else:
                reduced_window = max(3, int(round(window / stride)))
                if reduced_window % 2 == 0:
                    reduced_window += 1
                if reduced_window > reduced_series.size:
                    reduced_window = reduced_series.size
                    if reduced_window % 2 == 0:
                        reduced_window -= 1
                if reduced_window < 3:
                    seg_med, seg_mad = _rolling_median_and_mad(segment, window)
                else:
                    reduced_med, reduced_mad = _rolling_median_and_mad(
                        reduced_series, reduced_window
                    )
                    seg_med = np.repeat(reduced_med, stride)[:seg_len]
                    seg_mad = np.repeat(reduced_mad, stride)[:seg_len]
        local_medians[seg_start:seg_end] = seg_med
        local_mads[seg_start:seg_end] = seg_mad

    thresholds = np.full(signal.size, global_threshold, dtype=float)
    valid_local = np.isfinite(local_medians)
    adjusted_mads = np.where(
        valid_local & np.isfinite(local_mads) & (local_mads > 1e-6),
        local_mads,
        mad,
    )
    local_thresholds = local_medians + 2.5 * adjusted_mads
    local_thresholds = np.where(
        local_thresholds <= 0,
        np.where(local_medians > 0, local_medians * 1.5, global_threshold),
        local_thresholds,
    )
    np.copyto(thresholds, local_thresholds, where=valid_local)

    thresholds = np.clip(thresholds, 0.05, None)

    above_threshold = positive_velocity >= thresholds
    crossings = np.flatnonzero(~above_threshold[:-1] & above_threshold[1:])
    if crossings.size:
        candidate_onsets = crossings + 1
    else:
        candidate_onsets = np.arange(signal.size)

    min_distance = max(1, int(round(min_rr * fs)))
    max_distance = max(min_distance + 1, int(round(max_rr * fs)))
    max_search_window = min(0.45, 0.6 * max(min_rr, 0.2))
    search_horizon = max(min_distance, int(round(max_search_window * fs)))

    amplitude_span = float(np.percentile(signal, 95) - np.percentile(signal, 5))
    if signal.size > 1:
        noise_proxy = float(np.median(np.abs(np.diff(signal))))
    else:
        noise_proxy = 0.0
    adaptive_floor = max(amplitude_span * prominence_noise_factor, noise_proxy * 5.0)
    min_prominence = max(prominence_floor, adaptive_floor)

    downstroke_grace = max(1, int(round(0.02 * fs)))
    sustained_negative = max(1, int(round(0.02 * fs)))
    notch_guard = max(1, int(round(0.05 * fs)))

    if sp_signal is not None:
        candidate_peaks, _ = sp_signal.find_peaks(
            signal,
            distance=min_distance,
            prominence=min_prominence,
        )
        candidate_peaks = np.asarray(candidate_peaks, dtype=int)
    else:
        candidate_peaks, _ = find_prominent_peaks(
            signal,
            fs=fs,
            min_rr=min_rr,
            max_rr=max_rr,
            min_prominence=min_prominence,
        )

    if candidate_peaks.size == 0:
        return (
            np.array([], dtype=int),
            np.array([], dtype=float),
            np.array([], dtype=int),
            calibration_segments,
        )

    onset_indices = np.searchsorted(candidate_onsets, candidate_peaks, side="right") - 1
    valid_mask = onset_indices >= 0
    if valid_mask.any():
        onset_samples = candidate_onsets[onset_indices[valid_mask]]
        within_window = candidate_peaks[valid_mask] - onset_samples <= search_horizon
        valid_mask_indices = np.flatnonzero(valid_mask)
        keep = valid_mask_indices[within_window]
    else:
        keep = np.array([], dtype=int)

    if keep.size == 0:
        return (
            np.array([], dtype=int),
            np.array([], dtype=float),
            np.array([], dtype=int),
            calibration_segments,
        )

    candidate_peaks = candidate_peaks[keep]
    onset_samples = candidate_onsets[onset_indices[keep]]

    order = np.lexsort((-signal[candidate_peaks], onset_samples))
    sorted_onsets = onset_samples[order]
    _, unique_indices = np.unique(sorted_onsets, return_index=True)
    chosen = order[unique_indices]
    candidate_peaks = candidate_peaks[chosen]
    onset_samples = onset_samples[chosen]

    order = np.argsort(candidate_peaks, kind="mergesort")
    candidate_peaks = candidate_peaks[order]

    precomputed_prominences: Optional[np.ndarray]
    if candidate_peaks.size and sp_signal is not None:
        try:
            wlen = int(min(signal.size, 2 * max_distance))
            wlen = max(wlen, 3)
            if wlen % 2 == 0:
                wlen = max(3, wlen - 1)
            precomputed_prominences, _, _ = sp_signal.peak_prominences(
                signal,
                candidate_peaks,
                wlen=wlen,
            )
        except Exception:
            precomputed_prominences = None
    else:
        precomputed_prominences = None

    negative_mask = np.isfinite(velocity) & (velocity <= 0)
    if negative_mask.size >= sustained_negative:
        downstroke_kernel = np.ones(sustained_negative, dtype=int)
        downstroke_counts = np.convolve(
            negative_mask.astype(int), downstroke_kernel, mode="valid"
        )
    else:
        downstroke_counts = np.array([], dtype=int)

    def _estimate_prominence(peak_idx: int) -> Optional[float]:
        left = signal[max(0, peak_idx - max_distance) : peak_idx + 1]
        right = signal[peak_idx : min(signal.size, peak_idx + max_distance)]

        left = left[np.isfinite(left)]
        right = right[np.isfinite(right)]
        if left.size == 0 or right.size == 0:
            return None

        left_min = float(np.min(left))
        right_min = float(np.min(right))
        return float(signal[peak_idx] - max(left_min, right_min))

    def _estimate_notch(peak_idx: int) -> int:
        notch_idx = -1
        notch_start = peak_idx + 1
        notch_end = min(signal.size, peak_idx + int(round(0.45 * fs)))
        if notch_end > notch_start + 1:
            velocity_slice = velocity[notch_start:notch_end]
            zero_crossings = np.flatnonzero(
                (velocity_slice[:-1] < 0) & (velocity_slice[1:] >= 0)
            )
            if zero_crossings.size:
                zero_idx = notch_start + zero_crossings[0] + 1
                window_radius = max(1, int(round(0.02 * fs)))
                local_start = max(notch_start, zero_idx - window_radius)
                local_end = min(signal.size, zero_idx + window_radius)
                if local_end > local_start:
                    local_segment = signal[local_start:local_end]
                    if np.any(np.isfinite(local_segment)):
                        notch_rel = int(np.nanargmin(local_segment))
                        notch_idx = local_start + notch_rel
        return notch_idx

    def _legacy_has_sustained_downstroke(peak_idx: int) -> bool:
        window_end = min(signal.size, peak_idx + sustained_negative + downstroke_grace + 1)
        if window_end <= peak_idx:
            return False
        velocity_window = velocity[peak_idx:window_end]
        finite_velocity = velocity_window[np.isfinite(velocity_window)]
        if finite_velocity.size == 0:
            return False
        negative_mask = velocity_window <= 0
        if negative_mask.size < sustained_negative:
            return False
        kernel = np.ones(sustained_negative, dtype=int)
        sustained = np.convolve(negative_mask.astype(int), kernel, mode="valid")
        return bool(np.any(sustained >= sustained_negative))

    def _has_sustained_downstroke(peak_idx: int) -> bool:
        if downstroke_counts.size:
            window_end = min(
                signal.size, peak_idx + sustained_negative + downstroke_grace + 1
            )
            if window_end - peak_idx < sustained_negative:
                return False
            max_start = downstroke_counts.size - 1
            start = peak_idx
            if start > max_start:
                return False
            stop = min(window_end - sustained_negative, max_start)
            if stop < start:
                return False
            return bool(
                np.any(downstroke_counts[start : stop + 1] >= sustained_negative)
            )
        return _legacy_has_sustained_downstroke(peak_idx)

    peaks: List[int] = []
    prominences: List[float] = []
    notches: List[int] = []
    suppress_until = -1

    for idx, peak_idx in enumerate(candidate_peaks):
        if precomputed_prominences is not None:
            prominence = float(precomputed_prominences[idx])
            if not np.isfinite(prominence):
                continue
        else:
            prominence = _estimate_prominence(peak_idx)
            if prominence is None:
                continue
        if prominence < min_prominence:
            continue
        if not _has_sustained_downstroke(peak_idx):
            continue

        if peaks and peak_idx - peaks[-1] < min_distance:
            if signal[peak_idx] > signal[peaks[-1]]:
                peaks[-1] = peak_idx
                prominences[-1] = prominence
                notches[-1] = _estimate_notch(peak_idx)
                suppress_until = (
                    notches[-1] if notches[-1] >= 0 else peak_idx
                ) + notch_guard
            continue

        if peak_idx <= suppress_until:
            if peaks and signal[peak_idx] > signal[peaks[-1]]:
                peaks[-1] = peak_idx
                prominences[-1] = prominence
                notches[-1] = _estimate_notch(peak_idx)
                suppress_until = (
                    notches[-1] if notches[-1] >= 0 else peaks[-1]
                ) + notch_guard
            continue

        notch_idx = _estimate_notch(peak_idx)
        peaks.append(peak_idx)
        prominences.append(prominence)
        notches.append(notch_idx)
        suppress_until = (notch_idx if notch_idx >= 0 else peak_idx) + notch_guard

    if peaks:
        peak_diffs = np.diff(peaks) / fs if len(peaks) > 1 else np.array([], dtype=float)
        global_interval = float(np.median(peak_diffs)) if peak_diffs.size else math.nan
        cleaned_peaks: List[int] = [peaks[0]]
        cleaned_prom: List[float] = [prominences[0]]
        cleaned_notches: List[int] = [notches[0]]
        interval_history: List[float] = []
        for idx in range(1, len(peaks)):
            interval = float((peaks[idx] - cleaned_peaks[-1]) / fs)
            if interval_history:
                local_median = float(np.median(interval_history[-3:]))
            else:
                local_median = global_interval if not math.isnan(global_interval) else interval
            short_interval = local_median > 0 and interval < 0.4 * local_median
            weak_prominence = prominences[idx] <= 0.6 * cleaned_prom[-1]
            if short_interval or weak_prominence:
                continue
            cleaned_peaks.append(peaks[idx])
            cleaned_prom.append(prominences[idx])
            cleaned_notches.append(notches[idx])
            interval_history.append(interval)

        peaks = cleaned_peaks
        prominences = cleaned_prom
        notches = cleaned_notches

    if len(peaks) < 3:
        fallback_peaks, fallback_prom = find_prominent_peaks(
            signal,
            fs=fs,
            min_rr=min_rr,
            max_rr=max_rr,
            min_prominence=min_prominence,
        )
        if fallback_peaks.size:
            fallback_notches = np.full_like(fallback_peaks, -1, dtype=int)
            return fallback_peaks, fallback_prom, fallback_notches, calibration_segments

    return (
        np.asarray(peaks, dtype=int),
        np.asarray(prominences, dtype=float),
        np.asarray(notches, dtype=int),
        calibration_segments,
    )


def find_prominent_peaks(
    signal: np.ndarray,
    fs: float,
    min_rr: float = 0.3,
    max_rr: float = 2.0,
    min_prominence: Optional[float] = None,
) -> Tuple[np.ndarray, np.ndarray]:
    """Detect systolic peaks using a simple prominence-based heuristic.

    Parameters
    ----------
    signal:
        The (smoothed) arterial pressure signal.
    fs:
        Sampling frequency in Hz.
    min_rr / max_rr:
        Minimum and maximum plausible RR-intervals (seconds).
    min_prominence:
        Minimum prominence in mmHg required to accept a peak.  When not
        provided, it defaults to 20% of the signal's standard deviation.
    """

    if min_prominence is None:
        finite = signal[np.isfinite(signal)]
        if len(finite) == 0:
            raise ValueError("Signal contains no finite samples.")
        min_prominence = 0.2 * float(np.std(finite))

    min_distance = max(1, int(min_rr * fs))
    max_distance = max(1, int(max_rr * fs))

    peaks: List[int] = []
    prominences: List[float] = []
    last_peak = -max_distance

    for idx in range(1, len(signal) - 1):
        if signal[idx] <= signal[idx - 1] or signal[idx] < signal[idx + 1]:
            continue

        left_window = signal[max(0, idx - max_distance) : idx]
        right_window = signal[idx + 1 : min(len(signal), idx + max_distance + 1)]
        if left_window.size == 0 or right_window.size == 0:
            continue

        left_min = float(np.min(left_window))
        right_min = float(np.min(right_window))
        prominence = signal[idx] - max(left_min, right_min)
        if prominence < min_prominence:
            continue

        if idx - last_peak < min_distance:
            # Retain the peak with the larger prominence within the refractory period.
            if prominence > prominences[-1]:
                peaks[-1] = idx
                prominences[-1] = prominence
            continue

        peaks.append(idx)
        prominences.append(prominence)
        last_peak = idx

    return np.asarray(peaks, dtype=int), np.asarray(prominences, dtype=float)


def derive_beats(
    time: np.ndarray,
    pressure: np.ndarray,
    fs: float,
    *,
    config: ArtifactConfig,
) -> List[Beat]:
    """Derive systolic/diastolic/MAP landmarks from a continuous waveform."""

    if len(time) != len(pressure):
        raise ValueError("Time and pressure arrays must have the same length.")

    finite_mask = np.isfinite(pressure)
    if not np.any(finite_mask):
        raise ValueError("Pressure signal contains no valid samples.")

    pressure_filled = pressure.copy()
    missing_mask = ~finite_mask
    invalid_spans = _find_long_gaps(time, missing_mask, config.max_missing_gap)
    if not np.all(finite_mask):
        pressure_filled[~finite_mask] = np.interp(
            np.flatnonzero(~finite_mask),
            np.flatnonzero(finite_mask),
            pressure[finite_mask],
        )

    min_rr, max_rr = config.rr_bounds

    filtered = zero_phase_filter(pressure_filled, fs=fs)
    smoothed = smooth_signal(filtered, window_seconds=0.03, fs=fs)
    (
        systolic_indices,
        prominences,
        notch_indices,
        calibration_segments,
    ) = detect_systolic_peaks(
        smoothed,
        fs=fs,
        min_rr=min_rr,
        max_rr=max_rr,
        prominence_floor=config.min_prominence,
        prominence_noise_factor=config.prominence_noise_factor,
    )

    beats: List[Beat] = []
    prev_dia_sample: Optional[int] = None

    reset_boundaries = [end for _, end in calibration_segments]
    post_calibration_targets: set[int] = set()
    for _, end in calibration_segments:
        for candidate in systolic_indices:
            if candidate >= end:
                post_calibration_targets.add(candidate)
                break

    for idx, sys_idx in enumerate(systolic_indices):
        prev_sys = systolic_indices[idx - 1] if idx > 0 else None
        next_sys = systolic_indices[idx + 1] if idx + 1 < len(systolic_indices) else None

        notch_idx = notch_indices[idx] if idx < len(notch_indices) else -1

        if prev_sys is not None:
            search_start = prev_sys + max(1, int(round(0.05 * fs)))
        else:
            search_start = max(0, sys_idx - int(round(max_rr * fs)))

        last_reset = max(
            (boundary for boundary in reset_boundaries if boundary <= sys_idx),
            default=-1,
        )
        if last_reset >= 0:
            search_start = max(search_start, int(last_reset))

        search_end = sys_idx
        if next_sys is not None:
            search_end = min(search_end, next_sys - max(1, int(round(0.15 * fs))))

        search_start = max(0, min(search_start, sys_idx))
        search_end = max(search_start + 1, min(sys_idx + 1, search_end + 1))

        segment = smoothed[search_start:search_end]
        if segment.size == 0:
            continue
        dia_rel = int(np.argmin(segment))
        dia_idx = search_start + dia_rel

        systolic_time = float(time[sys_idx])
        raw_sys = pressure[sys_idx]
        raw_dia = pressure[dia_idx]
        systolic_pressure = float(raw_sys) if math.isfinite(raw_sys) else float(pressure_filled[sys_idx])
        diastolic_time = float(time[dia_idx])
        diastolic_pressure = float(raw_dia) if math.isfinite(raw_dia) else float(pressure_filled[dia_idx])
        notch_time = float(time[notch_idx]) if 0 <= notch_idx < len(time) else math.nan
        map_time = float((systolic_time + diastolic_time) / 2.0)
        area_map = math.nan
        integration_end = dia_idx
        if 0 <= notch_idx <= dia_idx:
            integration_end = notch_idx
        if prev_dia_sample is not None and prev_dia_sample < integration_end:
            start_idx = prev_dia_sample
            end_idx = integration_end + 1
            if end_idx - start_idx > 2:
                segment_time = time[start_idx:end_idx]
                segment_pressure = pressure_filled[start_idx:end_idx]
                area = float(np.trapezoid(segment_pressure, segment_time))
                duration = float(segment_time[-1] - segment_time[0])
                if duration > 0:
                    area_map = area / duration
                    map_time = float((segment_time[0] + segment_time[-1]) / 2.0)

        if math.isnan(area_map):
            map_pressure = diastolic_pressure + (systolic_pressure - diastolic_pressure) / 3.0
        else:
            map_pressure = area_map
        rr_interval = None
        if idx > 0:
            rr_interval = float(systolic_time - time[systolic_indices[idx - 1]])

        reasons: List[str] = []
        for start_time, end_time in invalid_spans:
            if start_time <= systolic_time <= end_time:
                reasons.append("long gap interpolation")
                break
        is_post_calibration = sys_idx in post_calibration_targets
        if is_post_calibration:
            reasons.append("post calibration gap")

        beats.append(
            Beat(
                systolic_time=systolic_time,
                systolic_pressure=systolic_pressure,
                diastolic_time=diastolic_time,
                diastolic_pressure=diastolic_pressure,
                notch_time=notch_time,
                map_time=map_time,
                map_pressure=map_pressure,
                rr_interval=rr_interval,
                systolic_prominence=float(prominences[idx]),
                is_artifact=False,
                artifact_reasons=reasons,
                is_post_calibration_recovery=is_post_calibration,
            )
        )

        prev_dia_sample = dia_idx

    apply_artifact_rules(beats, config=config)
    return beats


def _rolling_median_and_mad(values: np.ndarray, window: int) -> Tuple[np.ndarray, np.ndarray]:
    """Return rolling median and MAD for ``values`` using an odd-sized window."""

    if window % 2 == 0:
        raise ValueError("window must be odd to compute centred statistics")

    values = np.asarray(values, dtype=float)
    medians = np.full(values.shape, np.nan, dtype=float)
    mads = np.full(values.shape, np.nan, dtype=float)

    if values.size == 0:
        return medians, mads

    if bn is not None:
        medians_bn = bn.move_median(values, window=window, center=True, min_count=3)
        medians[:] = medians_bn

        deviations = np.abs(values - medians_bn)
        mad_bn = bn.move_median(deviations, window=window, center=True, min_count=3)
        mad_bn = np.asarray(mad_bn, dtype=float) * 1.4826
        fallback_mask = (mad_bn < 1e-3) | ~np.isfinite(mad_bn)
        if np.any(fallback_mask):
            std_bn = bn.move_std(
                values, window=window, center=True, ddof=0, min_count=3
            )
            std_bn = np.asarray(std_bn, dtype=float)
            mad_bn = mad_bn.copy()
            mad_bn[fallback_mask] = std_bn[fallback_mask]

        mads[:] = mad_bn
        return medians, mads

    if sliding_window_view is not None:
        half_window = window // 2
        padded = np.pad(values, (half_window, half_window), mode="constant", constant_values=np.nan)
        windows = sliding_window_view(padded, window_shape=window)
        finite_counts = np.sum(np.isfinite(windows), axis=1)
        valid_mask = finite_counts >= 3

        if not np.any(valid_mask):
            return medians, mads

        valid_windows = windows[valid_mask]
        medians_valid = np.nanmedian(valid_windows, axis=1)
        medians[valid_mask] = medians_valid

        deviations = np.abs(valid_windows - medians_valid[:, None])
        mad_valid = 1.4826 * np.nanmedian(deviations, axis=1)
        fallback_mask = (mad_valid < 1e-3) | ~np.isfinite(mad_valid)
        if np.any(fallback_mask):
            mad_valid[fallback_mask] = np.nanstd(valid_windows[fallback_mask], axis=1, ddof=0)

        mads[valid_mask] = mad_valid
        return medians, mads

    # Fallback path when sliding_window_view is unavailable (older NumPy).
    series = pd.Series(values, dtype=float)
    rolling = series.rolling(window, center=True, min_periods=3)
    medians_series = rolling.median()
    medians = medians_series.to_numpy(dtype=float)

    deviations = (series - medians_series).abs()
    mad_series = deviations.rolling(window, center=True, min_periods=3).median() * 1.4826
    mads = mad_series.to_numpy(dtype=float)

    fallback_mask = (mads < 1e-3) | ~np.isfinite(mads)
    if np.any(fallback_mask):
        std_series = rolling.std(ddof=0)
        std_values = std_series.to_numpy(dtype=float)
        mads[fallback_mask] = std_values[fallback_mask]

    return medians, mads


def _robust_central_tendency(values: np.ndarray) -> Tuple[float, float]:
    """Compute median and MAD with sensible fallbacks for empty arrays."""

    finite = values[np.isfinite(values)]
    if finite.size == 0:
        return math.nan, math.nan
    median = float(np.median(finite))
    deviations = np.abs(finite - median)
    mad = float(1.4826 * np.median(deviations)) if deviations.size else 0.0
    if mad < 1e-3:
        mad = float(np.std(finite, ddof=0))
    return median, mad


def apply_artifact_rules(beats: List[Beat], *, config: ArtifactConfig) -> None:
    """Flag beats that violate simple physiological heuristics."""

    if not beats:
        return

    systolic_values = np.array([b.systolic_pressure for b in beats], dtype=float)
    diastolic_values = np.array([b.diastolic_pressure for b in beats], dtype=float)
    rr_values = np.array([
        b.rr_interval if b.rr_interval is not None else math.nan for b in beats
    ])

    sys_med, sys_mad = _robust_central_tendency(systolic_values)
    dia_med, dia_mad = _robust_central_tendency(diastolic_values)
    rr_med, rr_mad = _robust_central_tendency(rr_values)

    sys_roll_med, sys_roll_mad = _rolling_median_and_mad(systolic_values, window=9)
    dia_roll_med, dia_roll_mad = _rolling_median_and_mad(diastolic_values, window=9)
    rr_roll_med, rr_roll_mad = _rolling_median_and_mad(rr_values, window=9)

    min_rr, max_rr = config.rr_bounds

    for idx, beat in enumerate(beats):
        severe_reasons: List[str] = []
        soft_reasons: List[str] = []

        if not math.isfinite(beat.systolic_pressure) or not math.isfinite(beat.diastolic_pressure):
            severe_reasons.append("non-finite pressure")
        if beat.systolic_pressure < 40 or beat.systolic_pressure > 260:
            severe_reasons.append("implausible systolic")
        if beat.diastolic_pressure < 20 or beat.diastolic_pressure > 160:
            severe_reasons.append("implausible diastolic")
        if beat.systolic_pressure - beat.diastolic_pressure < config.pulse_pressure_min:
            severe_reasons.append("low pulse pressure")

        sys_ref = sys_roll_med[idx] if math.isfinite(sys_roll_med[idx]) else sys_med
        sys_scale = sys_roll_mad[idx] if math.isfinite(sys_roll_mad[idx]) else sys_mad
        if math.isfinite(sys_ref) and math.isfinite(beat.systolic_pressure) and math.isfinite(sys_scale):
            limit = max(config.systolic_abs_floor, config.systolic_mad_multiplier * max(sys_scale, 1.0))
            if abs(beat.systolic_pressure - sys_ref) > limit:
                soft_reasons.append("systolic deviation")

        dia_ref = dia_roll_med[idx] if math.isfinite(dia_roll_med[idx]) else dia_med
        dia_scale = dia_roll_mad[idx] if math.isfinite(dia_roll_mad[idx]) else dia_mad
        if math.isfinite(dia_ref) and math.isfinite(beat.diastolic_pressure) and math.isfinite(dia_scale):
            limit = max(config.diastolic_abs_floor, config.diastolic_mad_multiplier * max(dia_scale, 1.0))
            if abs(beat.diastolic_pressure - dia_ref) > limit:
                soft_reasons.append("diastolic deviation")

        if beat.rr_interval is not None and math.isfinite(beat.rr_interval):
            if beat.rr_interval < min_rr * 0.7 or beat.rr_interval > max_rr * 1.3:
                severe_reasons.append("rr outside bounds")
            rr_ref = rr_roll_med[idx] if math.isfinite(rr_roll_med[idx]) else rr_med
            rr_scale = rr_roll_mad[idx] if math.isfinite(rr_roll_mad[idx]) else rr_mad
            if math.isfinite(rr_ref) and math.isfinite(rr_scale):
                limit = max(0.1, config.rr_mad_multiplier * max(rr_scale, 0.05))
                if abs(beat.rr_interval - rr_ref) > limit:
                    soft_reasons.append("rr deviation")

        if beat.systolic_prominence < config.min_prominence:
            soft_reasons.append("low prominence")

        combined_reasons = list(beat.artifact_reasons)
        if any(reason == "long gap interpolation" for reason in beat.artifact_reasons):
            severe_reasons.append("long gap interpolation")
        combined_reasons.extend(severe_reasons)
        combined_reasons.extend(soft_reasons)
        beat.is_artifact = bool(severe_reasons or len(soft_reasons) >= 2)
        if (
            beat.is_post_calibration_recovery
            and beat.is_artifact
            and not severe_reasons
        ):
            beat.is_artifact = False
            combined_reasons = [
                reason
                for reason in combined_reasons
                if reason not in soft_reasons
            ]
        beat.artifact_reasons = combined_reasons if beat.is_artifact else []


def plot_waveform(
    time: np.ndarray,
    pressure: np.ndarray,
    beats: Sequence[Beat],
    *,
    show: bool = True,
    save_path: Optional[Path] = None,
    max_points: Optional[int] = 50000,
) -> None:
    """Plot the waveform with annotated beat landmarks."""

    if plt is None:  # pragma: no cover - handled in main
        raise RuntimeError("matplotlib is required for plotting")

    plt.figure(figsize=(12, 6))
    time_values = np.asarray(time, dtype=float)
    pressure_values = np.asarray(pressure, dtype=float)

    if max_points is not None and max_points > 0 and time_values.size > max_points:
        stride = int(math.ceil(time_values.size / float(max_points)))
        stride = max(1, stride)
        downsampled_time = time_values[::stride]
        downsampled_pressure = pressure_values[::stride]
        if downsampled_time.size == 0 or downsampled_time[-1] != time_values[-1]:
            downsampled_time = np.append(downsampled_time, time_values[-1])
            downsampled_pressure = np.append(downsampled_pressure, pressure_values[-1])
        plot_time = downsampled_time
        plot_pressure = downsampled_pressure
    else:
        plot_time = time_values
        plot_pressure = pressure_values

    plt.plot(plot_time, plot_pressure, label="reBAP", color="tab:blue")

    systolic_times = [b.systolic_time for b in beats]
    systolic_pressures = [b.systolic_pressure for b in beats]
    diastolic_times = [b.diastolic_time for b in beats]
    diastolic_pressures = [b.diastolic_pressure for b in beats]
    map_times = [b.map_time for b in beats]
    map_pressures = [b.map_pressure for b in beats]

    artifacts = [idx for idx, beat in enumerate(beats) if beat.is_artifact]
    clean = [idx for idx, beat in enumerate(beats) if not beat.is_artifact]

    if clean:
        plt.scatter(
            np.array(systolic_times)[clean],
            np.array(systolic_pressures)[clean],
            color="tab:red",
            label="Systolic",
        )
        plt.scatter(
            np.array(diastolic_times)[clean],
            np.array(diastolic_pressures)[clean],
            color="tab:green",
            label="Diastolic",
        )
        plt.scatter(
            np.array(map_times)[clean],
            np.array(map_pressures)[clean],
            color="tab:orange",
            label="MAP",
        )

    if artifacts:
        plt.scatter(
            np.array(systolic_times)[artifacts],
            np.array(systolic_pressures)[artifacts],
            marker="x",
            color="k",
            label="Flagged beats",
        )

    plt.xlabel("Time (s)")
    plt.ylabel("Pressure (mmHg)")
    plt.title("Continuous blood pressure with beat landmarks")
    plt.legend()
    plt.tight_layout()

    if save_path is not None:
        plt.savefig(save_path, dpi=150)

    if show:
        plt.show()
    else:
        plt.close()


def summarise_beats(beats: Sequence[Beat]) -> DataFrame:
    """Build a DataFrame summarising the detected beats."""

    base_columns = {
        "systolic_time": pd.Series(dtype=float),
        "systolic_pressure": pd.Series(dtype=float),
        "diastolic_time": pd.Series(dtype=float),
        "diastolic_pressure": pd.Series(dtype=float),
        "notch_time": pd.Series(dtype=float),
        "map_time": pd.Series(dtype=float),
        "map_pressure": pd.Series(dtype=float),
        "rr_interval": pd.Series(dtype=float),
        "prominence": pd.Series(dtype=float),
        "artifact": pd.Series(dtype=bool),
        "artifact_reasons": pd.Series(dtype=object),
    }

    records = []
    for beat in beats:
        records.append(
            {
                "systolic_time": beat.systolic_time,
                "systolic_pressure": beat.systolic_pressure,
                "diastolic_time": beat.diastolic_time,
                "diastolic_pressure": beat.diastolic_pressure,
                "notch_time": beat.notch_time,
                "map_time": beat.map_time,
                "map_pressure": beat.map_pressure,
                "rr_interval": beat.rr_interval,
                "prominence": beat.systolic_prominence,
                "artifact": beat.is_artifact,
                "artifact_reasons": ", ".join(beat.artifact_reasons) if beat.artifact_reasons else "",
            }
        )
    if not records:
        return pd.DataFrame(base_columns)

    return pd.DataFrame.from_records(records)

def _resample_even_grid(times: np.ndarray, values: np.ndarray, fs: float) -> Tuple[np.ndarray, np.ndarray]:
    """Interpolate ``values`` onto an even time grid at ``fs`` Hz."""

    if times.size == 0 or values.size == 0 or fs <= 0:
        return np.array([]), np.array([])

    start = float(times[0])
    end = float(times[-1])
    if end <= start:
        return np.array([]), np.array([])

    grid = np.arange(start, end, 1.0 / fs, dtype=float)
    finite_mask = np.isfinite(values)
    if not np.any(finite_mask):
        return grid, np.full_like(grid, np.nan)

    interp_values = np.interp(grid, times[finite_mask], values[finite_mask])
    return grid, interp_values


def compute_bpv_psd(
    beat_times: np.ndarray,
    beat_values: np.ndarray,
    *,
    resample_fs: float,
    nperseg: int = 256,
) -> Optional[Tuple[np.ndarray, np.ndarray]]:
    """Compute Welch PSD for beat-to-beat series when SciPy is present."""

    if sp_signal is None:
        return None

    grid_time, resampled = _resample_even_grid(beat_times, beat_values, resample_fs)
    if grid_time.size < nperseg:
        return None

    try:
        freqs, power = sp_signal.welch(resampled, fs=resample_fs, nperseg=min(nperseg, len(resampled)))
    except Exception:
        return None

    return freqs, power


def _bandpower(freqs: np.ndarray, power: np.ndarray, band: Tuple[float, float]) -> float:
    """Integrate Welch spectrum within ``band`` (in Hz)."""

    low, high = band
    mask = (freqs >= low) & (freqs <= high)
    if not np.any(mask):
        return math.nan
    return float(np.trapezoid(power[mask], freqs[mask]))


def compute_rr_metrics(rr_intervals: np.ndarray, *, pnn_threshold: float) -> Dict[str, float]:
    """Compute summary statistics for RR-intervals."""

    rr = rr_intervals[np.isfinite(rr_intervals)]
    if rr.size == 0:
        return {
            "mean": math.nan,
            "sd": math.nan,
            "cv": math.nan,
            "rmssd": math.nan,
            "arv": math.nan,
            "pnn": math.nan,
        }

    diffs = np.diff(rr)
    rmssd = float(np.sqrt(np.mean(diffs**2))) if diffs.size else math.nan
    arv = float(np.mean(np.abs(diffs))) if diffs.size else math.nan
    if diffs.size:
        pnn = float(np.mean(np.abs(diffs) > pnn_threshold))
    else:
        pnn = math.nan

    mean_rr = float(np.mean(rr))
    sd_rr = float(np.std(rr, ddof=1)) if rr.size > 1 else 0.0
    cv_rr = float(sd_rr / mean_rr) if mean_rr else math.nan

    return {
        "mean": mean_rr,
        "sd": sd_rr,
        "cv": cv_rr,
        "rmssd": rmssd,
        "arv": arv,
        "pnn": pnn,
    }


def print_column_overview(frame: DataFrame) -> None:
    """Display column names, dtypes, and first numeric samples to aid selection."""

    print("\nAvailable columns:")
    for name in frame.columns:
        series = frame[name]
        dtype = str(series.dtype)
        if pd.api.types.is_numeric_dtype(series):
            sample = series.dropna().astype(float).head(3).to_list()
            preview = ", ".join(f"{value:.3f}" for value in sample)
        else:
            sample = series.dropna().astype(str).head(3).to_list()
            preview = ", ".join(sample)
        if not preview:
            preview = "(no finite samples)"
        print(f"  - {name} [{dtype}]: {preview}")


def select_file_via_dialog() -> Optional[Path]:
    """Open a file picker to select a waveform export."""

    if tk is None or filedialog is None:
        print("Tkinter is not available; please supply a file path as an argument.")
        return None

    root = tk.Tk()
    root.withdraw()
    file_types = [("Text files", "*.txt"), ("All files", "*")]
    filename = filedialog.askopenfilename(title="Select continuous BP export", filetypes=file_types)
    root.destroy()

    if not filename:
        return None
    return Path(filename)


def build_arg_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(description="Explore beat-to-beat arterial pressure waveforms.")
    parser.add_argument("file", nargs="?", help="Path to the exported waveform text file.")
    parser.add_argument("--column", default="reBAP", help="Name of the arterial pressure column to analyse.")
    parser.add_argument("--time-column", default="Time", help="Name of the time column (defaults to 'Time').")
    parser.add_argument("--min-rr", type=float, default=0.3, help="Minimum RR interval in seconds.")
    parser.add_argument("--max-rr", type=float, default=2.0, help="Maximum RR interval in seconds.")
    parser.add_argument("--min-prominence", type=float, default=8.0, help="Minimum systolic prominence (mmHg).")
    parser.add_argument(
        "--prominence-factor",
        type=float,
        default=0.18,
        help="Multiplier for adaptive prominence threshold based on noise.",
    )
    parser.add_argument(
        "--systolic-mad-multiplier",
        type=float,
        default=4.5,
        help="MAD multiplier for systolic outlier detection.",
    )
    parser.add_argument(
        "--diastolic-mad-multiplier",
        type=float,
        default=4.5,
        help="MAD multiplier for diastolic outlier detection.",
    )
    parser.add_argument(
        "--rr-mad-multiplier",
        type=float,
        default=3.5,
        help="MAD multiplier for RR-interval outlier detection.",
    )
    parser.add_argument("--pulse-pressure-min", type=float, default=10.0, help="Minimum pulse pressure (mmHg).")
    parser.add_argument(
        "--systolic-abs-floor",
        type=float,
        default=15.0,
        help="Absolute systolic deviation floor in mmHg.",
    )
    parser.add_argument(
        "--diastolic-abs-floor",
        type=float,
        default=12.0,
        help="Absolute diastolic deviation floor in mmHg.",
    )
    parser.add_argument(
        "--max-gap",
        type=float,
        default=10.0,
        help="Maximum tolerated gap (s) before interpolated beats are flagged.",
    )
    parser.add_argument(
        "--pnn-threshold",
        type=float,
        default=50.0,
        help="pNN threshold in milliseconds for RR statistics.",
    )
    parser.add_argument("--psd", action="store_true", help="Compute Welch PSD for clean beat series.")
    parser.add_argument("--psd-fs", type=float, default=4.0, help="Resampling frequency for PSD (Hz).")
    parser.add_argument("--psd-nperseg", type=int, default=256, help="nperseg parameter for Welch PSD.")
    parser.add_argument(
        "--separator",
        help="Override the column separator if auto-detection fails (e.g., ',' or \\t).",
    )
    parser.add_argument(
        "--plot-max-points",
        type=int,
        default=50000,
        help="Maximum waveform samples to render (set to 0 to disable downsampling).",
    )
    parser.add_argument("--out", type=Path, help="Optional path to export the beat summary CSV.")
    parser.add_argument("--savefig", type=Path, help="Save the plot to this path instead of/as well as showing it.")
    parser.add_argument("--no-plot", action="store_true", help="Skip interactive plot display.")
    return parser


def main(argv: Sequence[str]) -> int:
    parser = build_arg_parser()
    args = parser.parse_args(argv[1:])

    if args.min_rr >= args.max_rr:
        parser.error("--min-rr must be less than --max-rr")

    if args.plot_max_points is not None and args.plot_max_points < 0:
        parser.error("--plot-max-points must be zero or a positive integer")

    separator_override: Optional[str] = None
    if args.separator:
        raw_separator = args.separator
        try:
            raw_separator = bytes(raw_separator, "utf-8").decode("unicode_escape")
        except Exception:
            pass
        lowered = raw_separator.lower()
        if lowered == "tab":
            raw_separator = "\t"
        elif lowered == "space":
            raw_separator = " "
        separator_override = raw_separator if raw_separator else None

    if args.file:
        file_path = Path(args.file)
    else:
        file_path = select_file_via_dialog()
        if file_path is None:
            print("No file selected. Exiting.")
            return 1

    if not file_path.exists():
        print(f"File not found: {file_path}")
        return 1

    try:
        preview = _scan_bp_file(file_path, separator_override=separator_override)
    except Exception as exc:  # pragma: no cover - interactive feedback
        print(f"Failed to load file preview: {exc}")
        return 1

    time_default = _default_column_selection(preview.column_names, args.time_column, fallback="Time")
    remaining_columns = [name for name in preview.column_names if name != time_default]
    pressure_default = _default_column_selection(
        remaining_columns if remaining_columns else preview.column_names,
        args.column if args.column != time_default else None,
        fallback="reBAP",
    )

    selected_time_column: Optional[str] = None
    selected_pressure_column: Optional[str] = None
    dialog_shown = False
    dialog_result: Optional[ImportDialogResult] = None

    if tk is not None:
        try:
            dialog_result = launch_import_configuration_dialog(
                file_path,
                preview,
                time_default=time_default,
                pressure_default=pressure_default,
                separator_override=separator_override,
            )
            dialog_shown = True
        except Exception as exc:  # pragma: no cover - interactive feedback
            print(f"Failed to open import configuration dialog: {exc}")
            dialog_result = None
        if dialog_result:
            preview = dialog_result.preview
            separator_override = dialog_result.separator
            selected_time_column = dialog_result.time_column
            selected_pressure_column = dialog_result.pressure_column

    print(f"Loaded file preview: {file_path}")
    print("\nColumn preview (first 200 rows loaded for preview):")
    preview_frame = preview.preview
    if preview_frame is None:
        try:
            preview_frame = _build_preview_dataframe(
                file_path,
                column_names=preview.column_names,
                skiprows=preview.skiprows,
                separator=preview.detected_separator,
                max_rows=preview.preview_rows,
            )
            preview.preview = preview_frame
        except Exception as exc:  # pragma: no cover - interactive feedback
            print(f"  Unable to build preview table: {exc}")
            preview_frame = None
    if preview_frame is not None:
        print_column_overview(preview_frame)
    else:
        print("  Preview will be available after import.")

    if selected_time_column is None or selected_pressure_column is None:
        selected_time_column, selected_pressure_column = select_time_pressure_columns(
            preview,
            requested_time=args.time_column,
            requested_pressure=args.column,
            allow_gui=not dialog_shown,
        )

    try:
        frame, metadata = load_bp_file(
            file_path,
            preview=preview,
            time_column=selected_time_column,
            pressure_column=selected_pressure_column,
            separator=separator_override,
            import_settings=dialog_result,
        )
    except Exception as exc:  # pragma: no cover - interactive feedback
        print(f"Failed to load file: {exc}")
        return 1

    print(f"Loaded columns: time='{selected_time_column}', pressure='{selected_pressure_column}'")

    if selected_time_column in frame.columns:
        time_series = pd.to_numeric(
            frame[selected_time_column], errors="coerce", downcast="float"
        ).astype(np.float32, copy=False)
        time = time_series.to_numpy()
    else:
        time = None

    pressure_series = pd.to_numeric(
        frame[selected_pressure_column], errors="coerce", downcast="float"
    ).astype(np.float32, copy=False)
    pressure = pressure_series.to_numpy()

    interval = parse_interval(metadata, frame)
    fs = 1.0 / interval if interval > 0 else 1.0

    if time is None or not np.any(np.isfinite(time)):
        time = np.arange(len(frame), dtype=np.float32) * np.float32(interval)

    config = ArtifactConfig(
        systolic_mad_multiplier=args.systolic_mad_multiplier,
        diastolic_mad_multiplier=args.diastolic_mad_multiplier,
        rr_mad_multiplier=args.rr_mad_multiplier,
        systolic_abs_floor=args.systolic_abs_floor,
        diastolic_abs_floor=args.diastolic_abs_floor,
        pulse_pressure_min=args.pulse_pressure_min,
        rr_bounds=(args.min_rr, args.max_rr),
        min_prominence=args.min_prominence,
        prominence_noise_factor=args.prominence_factor,
        max_missing_gap=args.max_gap,
    )

    try:
        beats = derive_beats(time, pressure, fs=fs, config=config)
    except Exception as exc:  # pragma: no cover - interactive feedback
        print(f"Beat detection failed: {exc}")
        return 1

    summary = summarise_beats(beats)

    if args.out:
        try:
            summary.to_csv(args.out, index=False)
            print(f"Beat summary exported to {args.out}")
        except Exception as exc:  # pragma: no cover - filesystem feedback
            print(f"Failed to save summary CSV: {exc}")

    clean_beats = summary[~summary["artifact"]].copy()
    if not clean_beats.empty:
        print("\nDetected beats (first clean entries):")
        print(
            clean_beats[
                [
                    "systolic_time",
                    "systolic_pressure",
                    "diastolic_pressure",
                    "map_pressure",
                    "rr_interval",
                ]
            ].head()
        )
        print()
        print(
            "Averages — Systolic: "
            f"{clean_beats['systolic_pressure'].mean():.1f} mmHg, Diastolic: "
            f"{clean_beats['diastolic_pressure'].mean():.1f} mmHg, MAP: "
            f"{clean_beats['map_pressure'].mean():.1f} mmHg"
        )

        rr_metrics = compute_rr_metrics(
            clean_beats["rr_interval"].to_numpy(dtype=float),
            pnn_threshold=args.pnn_threshold / 1000.0,
        )
        print(
            "RR metrics — mean: "
            f"{rr_metrics['mean']:.3f}s, SD: {rr_metrics['sd']:.3f}s, CV: {rr_metrics['cv']:.3f}, "
            f"RMSSD: {rr_metrics['rmssd']:.3f}s, ARV: {rr_metrics['arv']:.3f}s, "
            f"pNN>{args.pnn_threshold:.0f}ms: {rr_metrics['pnn'] * 100 if not math.isnan(rr_metrics['pnn']) else float('nan'):.1f}%"
        )
    else:
        print("No clean beats detected; all beats were flagged as artefacts.")

    if summary["artifact"].any():
        print()
        print(
            f"{summary['artifact'].sum()} beats were flagged as potential artefacts."
        )

    if args.psd:
        if sp_signal is None:
            print("SciPy is not installed; skipping PSD computation.")
        elif clean_beats.empty:
            print("PSD skipped because no clean beats are available.")
        else:
            beat_times = clean_beats["systolic_time"].to_numpy(dtype=float)
            if np.any(np.diff(beat_times) > config.max_missing_gap):
                print(
                    "PSD skipped because long gaps (>"
                    f"{config.max_missing_gap}s) remain after artefact removal."
                )
            else:
                for label, column in [
                    ("SBP", "systolic_pressure"),
                    ("DBP", "diastolic_pressure"),
                    ("MAP", "map_pressure"),
                ]:
                    result = compute_bpv_psd(
                        beat_times,
                        clean_beats[column].to_numpy(dtype=float),
                        resample_fs=args.psd_fs,
                        nperseg=args.psd_nperseg,
                    )
                    if result is None:
                        print(f"PSD for {label} unavailable (insufficient data or SciPy error).")
                        continue
                    freqs, power = result
                    lf = _bandpower(freqs, power, (0.04, 0.15))
                    hf = _bandpower(freqs, power, (0.15, 0.4))
                    hf = hf if hf > 0 else math.nan
                    lf_hf = lf / hf if not math.isnan(hf) and hf != 0 else math.nan
                    print(
                        f"{label} PSD — LF: {lf:.3f} mmHg^2, HF: {hf:.3f} mmHg^2, LF/HF: {lf_hf:.3f}"
                    )

    show_plot = not args.no_plot
    if plt is None and (show_plot or args.savefig):
        print("matplotlib is required to visualise or save the waveform plot.")
    elif show_plot or args.savefig:
        plot_waveform(
            time,
            pressure,
            beats,
            show=show_plot,
            save_path=args.savefig,
            max_points=None if args.plot_max_points == 0 else args.plot_max_points,
        )

    return 0


if __name__ == "__main__":
    raise SystemExit(main(sys.argv))
